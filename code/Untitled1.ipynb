{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb51a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -m MODE -s SETTINGS [-n MODEL] [-o OUTPUT]\n",
      "                             [-k SCORE_KEY]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -m/--mode, -s/--settings\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aishw\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from IPython import embed\n",
    "\n",
    "def convert(o):\n",
    "    if isinstance(o, np.int64): return int(o)  \n",
    "    raise TypeError\n",
    "\n",
    "def select_hyperparams(config, output_name, model, is_arc, score_key='f_macro'):\n",
    "    ### make directories\n",
    "    config_path, checkpoint_path, result_path = make_dirs(config)\n",
    "\n",
    "    setup_params = ['tune_params', 'num_search_trials', 'dir_name']\n",
    "    model_params = set()\n",
    "    for p in config:\n",
    "        if p in setup_params or ('range' in p or 'algo' in p or 'type' in p or p.startswith('CON')): continue\n",
    "        model_params.add(p)\n",
    "    print(\"[model params] {}\".format(model_params))\n",
    "\n",
    "    score_lst = []\n",
    "    time_lst = []\n",
    "    best_epoch_lst = []\n",
    "    tn2vals = dict()\n",
    "    for trial_num in range(int(config['num_search_trials'])):\n",
    "        ### sample values\n",
    "        print(\"[trial {}] Starting...\".format(trial_num))\n",
    "        print(\"[trial {}] sampling parameters in {}\".format(trial_num, config['tune_params']))\n",
    "\n",
    "        constraints_OK = False\n",
    "        while not constraints_OK:\n",
    "            p2v = sample_values(trial_num)\n",
    "            constraints_OK = check_constraints(config, p2v)\n",
    "        tn2vals[trial_num] = p2v\n",
    "\n",
    "        ### construct the appropriate config file\n",
    "        config_file_name = config_path + 'config-{}.txt'.format(trial_num)\n",
    "        print(\"[trial {}] writing configuration to {}\".format(trial_num, config_file_name))\n",
    "        print(\"[trial {}] checkpoints to {}\".format(trial_num, checkpoint_path))\n",
    "        print(\"[trial {}] results to {}\".format(trial_num, result_path))\n",
    "        f = open(config_file_name, 'w')\n",
    "        model_name = '{}_t{}'.format(config['name'], trial_num)\n",
    "        f.write('name:{}\\n'.format(model_name)) # include trial number in name\n",
    "        f.write('ckp_path:{}\\n'.format(checkpoint_path)) # checkpoint save location\n",
    "        f.write('res_path:{}\\n'.format(result_path)) # results save location\n",
    "        for p in model_params:\n",
    "            if p == 'name': continue\n",
    "            f.write('{}:{}\\n'.format(p, config[p]))\n",
    "        for p in p2v:\n",
    "            f.write('{}:{}\\n'.format(p, p2v[p]))\n",
    "        f.flush()\n",
    "\n",
    "        ### run the script\n",
    "        print(\"[trial {}] running cross validation\".format(trial_num))\n",
    "        start_time = time.time()\n",
    "        if model == 'adv':\n",
    "            os.system(\"./adv_train.sh 1 {} 0 {} > {}log_t{}.txt\".format(config_file_name, score_key, result_path, trial_num))\n",
    "        elif model == 'bicond':\n",
    "            os.system(\"./bicond.sh {} {} > {}log_t{}.txt\".format(config_file_name, score_key, result_path, trial_num))\n",
    "        else:\n",
    "            print(\"ERROR: model {} is not supported\".format(model))\n",
    "            sys.exit(1)\n",
    "        script_time = (time.time() - start_time) / 60.\n",
    "        print(\"[trial {}] running on ARC took {:.4f} minutes\".format(trial_num, script_time))\n",
    "\n",
    "        ### process the result and update information on best\n",
    "        if model == 'adv':\n",
    "            res_f = open('{}{}_t{}-{}.top5_{}.txt'.format(result_path, config['name'], trial_num, config['enc'], score_key), 'r')\n",
    "        else:\n",
    "            res_f = open('{}{}_t{}.top5_{}.txt'.format(result_path, config['name'], trial_num, score_key), 'r')\n",
    "        res_lines = res_f.readlines()\n",
    "        score_lst.append(res_lines[-2].strip().split(':')[1])\n",
    "        time_lst.append(script_time)\n",
    "        best_epoch_lst.append(res_lines[-3].strip().split(':')[1])\n",
    "\n",
    "        print(\"[trial {}] Done.\".format(trial_num))\n",
    "        print()\n",
    "\n",
    "    ### save the resulting scores and times, for calculating the expected validation f1\n",
    "    data = []\n",
    "    for ti in tn2vals:\n",
    "        data.append([ti, score_lst[ti], time_lst[ti], best_epoch_lst[ti], json.dumps(tn2vals[ti], default=convert)])\n",
    "    df = pd.DataFrame(data, columns=['trial_num', 'avg_score', 'time', 'best_epoch', 'param_vals'])\n",
    "    df.to_csv('data/model_results/{}-{}trials/{}'.format(config['dir_name'], config['num_search_trials'],\n",
    "                                                      output_name), index=False)\n",
    "    print(\"results to {}\".format(output_name))\n",
    "\n",
    "\n",
    "def parse_config(fname):\n",
    "    f = open(fname, 'r')\n",
    "    lines = f.readlines()\n",
    "    n2info = dict()\n",
    "    for l in lines:\n",
    "        n, info = l.strip().split(':')\n",
    "        n2info[n] = info\n",
    "\n",
    "    n2info['tune_params'] = n2info['tune_params'].split(',')\n",
    "    for p in n2info['tune_params']:\n",
    "        t = n2info['{}_type'.format(p)]\n",
    "        n2info['{}_range'.format(p)] = list(map(lambda x: int(x) if t == 'int' else\n",
    "                                                    float(x) if t == 'float' else x,\n",
    "                                                    n2info['{}_range'.format(p)].split('-')))\n",
    "    return n2info\n",
    "\n",
    "\n",
    "def sample_values(trial_num):\n",
    "    p2v = dict()\n",
    "    for p in config['tune_params']:\n",
    "        a = config['{}_algo'.format(p)]\n",
    "        if a == 'selection':        #To select in order from a list of hyperparam values\n",
    "            p2v[p] = config['{}_range'.format(p)][trial_num]\n",
    "        elif a == 'choice':         #To randomly select any value from a list of hyperparam values\n",
    "            p2v[p] = np.random.choice(config['{}_range'.format(p)])\n",
    "        else:                       #To randomly select a value from a given range\n",
    "            min_v, max_v = config['{}_range'.format(p)]\n",
    "            if a == 'loguniform':\n",
    "                p2v[p] = loguniform.rvs(min_v, max_v)\n",
    "            elif a == 'uniform-integer':\n",
    "                p2v[p] = np.random.randint(min_v, max_v + 1)\n",
    "            elif a == 'uniform-float':\n",
    "                p2v[p] = uniform.rvs(min_v, max_v)\n",
    "            else:\n",
    "                print(\"ERROR: sampling method specified as {}\".format(a))\n",
    "\n",
    "    return p2v\n",
    "\n",
    "\n",
    "def check_constraints(n2info, p2v):\n",
    "    constraints_OK = True\n",
    "    for n in n2info:\n",
    "        if not n.startswith('CON'): continue\n",
    "        eq = n2info[n].split('#') # equations should be in format param1#symbol#param2\n",
    "        if len(eq) == 3:\n",
    "            con_res = parse_equation(p2v[eq[0]], eq[1], p2v[eq[2]])\n",
    "        elif len(eq) == 4:\n",
    "            if eq[0] in p2v:\n",
    "                v1 = p2v[eq[0]]\n",
    "                s = eq[1]\n",
    "                v2 = float(eq[2]) * p2v[eq[3]]\n",
    "            else:\n",
    "                v1 = float(eq[0]) * p2v[eq[1]]\n",
    "                s = eq[2]\n",
    "                v2 = p2v[eq[3]]\n",
    "            con_res = parse_equation(v1, s, v2)\n",
    "        else:\n",
    "            print(\"ERROR: equation not parsable {}\".format(eq))\n",
    "            sys.exit(1)\n",
    "        constraints_OK = con_res and constraints_OK\n",
    "    return constraints_OK\n",
    "\n",
    "\n",
    "def parse_equation(v1, s, v2):\n",
    "    if s == '<': return v1 < v2\n",
    "    elif s == '<=': return v1 <= v2\n",
    "    elif s == '=': return v1 == v2\n",
    "    elif s == '!=': return v1 != v2\n",
    "    elif s == '>': return v1 > v2\n",
    "    elif s == '>=': return v1 >= v2\n",
    "    else:\n",
    "        print(\"ERROR: symbol {} not recognized\".format(s))\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def make_dirs(config):\n",
    "    config_path = 'data/config/{}-{}trials/'.format(config['dir_name'],\n",
    "                                                    config['num_search_trials'])\n",
    "    checkpoint_path = 'data/checkpoints/{}-{}trials/'.format(config['dir_name'],\n",
    "                                                             config['num_search_trials'])\n",
    "    result_path = 'data/model_results/{}-{}trials/'.format(config['dir_name'],\n",
    "                                             config['num_search_trials'])\n",
    "    for p_name, p_path in [('config_path', config_path), ('ckp_path', checkpoint_path),\n",
    "                           ('result_path', result_path)]:\n",
    "        if not os.path.exists(p_path):\n",
    "            os.makedirs(p_path)\n",
    "        else:\n",
    "            print(\"[{}] Directory {} already exists!\".format(p_name, p_path))\n",
    "            sys.exit(1)\n",
    "    return config_path, checkpoint_path, result_path\n",
    "\n",
    "\n",
    "def remove_dirs(config):\n",
    "    config_path = 'data/config/{}-{}trials/'.format(config['dir_name'],\n",
    "                                                    config['num_search_trials'])\n",
    "    checkpoint_path = 'data/checkpoints/{}-{}trials/'.format(config['dir_name'],\n",
    "                                                             config['num_search_trials'])\n",
    "    result_path = 'data/model_results/{}-{}trials/'.format(config['dir_name'],\n",
    "                                             config['num_search_trials'])\n",
    "    for p_name, p_path in [('config_path', config_path), ('ckp_path', checkpoint_path),\n",
    "                           ('result_path', result_path)]:\n",
    "        if not os.path.exists(p_path):\n",
    "            print(\"[{}] directory {} doesn't exist\".format(p_name, p_path))\n",
    "            continue\n",
    "        else:\n",
    "            print(\"[{}] removing all files from {}\".format(p_name, p_path))\n",
    "            for fname in os.listdir(p_path):\n",
    "                os.remove(os.path.join(p_path, fname))\n",
    "            print(\"[{}] removing empty directory\".format(p_name))\n",
    "            os.rmdir(p_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-m', '--mode', help='What to do', required=True)\n",
    "    parser.add_argument('-s', '--settings', help='Name of the file containing hyperparam info', required=True)\n",
    "    # model_name should be bert-text-level or adv or bicond currently and is to be specified when is_arc is True.\n",
    "    parser.add_argument('-n', '--model', help='Name of the model to run', required=False, default='adv')\n",
    "    parser.add_argument('-o', '--output', help='Name of the output file (full path)', required=False,\n",
    "                        default='trial_results.csv')\n",
    "    parser.add_argument('-k', '--score_key', help='Score key for optimization', required=False, default='f_macro')\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "    config = parse_config(args['settings'])\n",
    "\n",
    "    if args['mode'] == '1':\n",
    "        ## run hyperparam search\n",
    "        remove_dirs(config)\n",
    "        select_hyperparams(config, args['output'], args['model'], is_arc=('arc' in args['settings'] or 'twitter' in args['settings']), score_key=args['score_key'])\n",
    "    elif args['mode'] == '2':\n",
    "        ## remove directories\n",
    "        remove_dirs(config)\n",
    "    else:\n",
    "        print(\"ERROR. exiting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50571b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "angular2html\n",
    "python train_and_eval_model.py --mode \"eval\" --config_file data/config-0.txt \n",
    "--trn_data data/twitter_testDT_seenval/development_setup/train.csv \n",
    "--dev_data data/twitter_testDT_seenval/test_setup/test.csv \n",
    "--saved_model_file_name data/checkpoints/DT_checkpoint.tar \n",
    "--topics_vocab twitter-topic-TRN-semi-sup.vocab.pkl --mode eval\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
